## 1.常见的损失函数

```
分类任务中常用的损失函数
二分类损失函数：BinaryCrossentropy
多分类损失函数：CategoricalCrossentropy
回归任务中常用的损失函数
MAE
MSE
```

## 2.初始化的几种方式，以及从什么分布中取点

```
1.Xavier初始化  在（-1/d,1/根号d）均匀分布中生成当前神经元的权重，d为每个神经元的输入数量
2.He初始化 （最常用）正态化的he初始化是以0为中心，标准差为stddev=sqrt(2/fan_in)的截断正态分布中抽取样本，其中fan_in是输入神经元的个数
   标准化的he初始化从[-limit,limit]中的均匀分布中抽取样本，其中limit是sqrt(6/fan_in)，其中fan_in输入神经元的个数
3.随机初始化  均值为0 标准差是1的高斯分布中取样
```

## 3.什么是前向传播

```
前向传播主要作用是将输入数据通过神经网络的各个层，最终得到输出结果。在前线传播过程中，输入数据首先经过输入层，然后通过隐藏层，最终到达输出层。在每个层中，输入数据都会与该层中的权重和偏置进行计算，得到该层的输出结果，并作为下一层的输入。（y=wx+b）
```

## 4.什么是反向传播

```
反向传播的基本思想是将神经网络的输出误差反向传播到每一层神经元，计算每个神经元的误差贡献，并根据这些误差贡献来调整权重.反向传播的关键在于利用链式法则计算每个神经元的误差贡献,反向传播算法结合使用梯度下降法则来进行权重的调整，通过沿着误差曲面的负梯度方向进行迭代，以找到误差最小的权重值。
```

## 5.梯度下降的原理介绍

```
梯度下降算法的核心思想是在目标函数的梯度方向上寻找最小值。具体来说，对于一个给定的初始点，梯度下降算法计算目标函数的梯度，并沿着梯度的负方向进行迭代更新。通过不断地迭代，梯度下降算法可以逐渐逼近目标函数的最小值点
```

## 6.链式法则介绍

```
主要用于计算复合函数的导数。它表示的是对于复合函数，其导数可以分解为各个组成函数的导数的乘积。

具体来说，如果有一个复合函数 y = f(g(x))，其中 g(x) 是内函数，f(u) 是外函数，那么根据链式法则，y 的导数可以表示为：

(dy/dx) = (dy/du) * (du/dx)

其中，(dy/du) 表示 y 关于 u 的导数，(du/dx) 表示 u 关于 x 的导数。如果一个函数是由两个或多个函数组合而成，链式法则可以帮助我们找到这个复合函数的导数
```

## 7.列出几种梯度下降的优化算法

```
1.批量梯度下降（BGD）：每次迭代时需要计算每个样本上损失函数的梯度并求和。缺点：计算量大，迭代速度慢。优点：全局最优化
2.随机梯度下降（SGD）：每次迭代时只随机采集一个样本，计算该样本损失函数的梯度并更新参数。缺点：准确率下降、存在噪音、非全局最优化。优点：训练速度快、支持在线学习
3.小批量梯度下降（MBGD）：每次迭代时，我们随机抽取一小部分训练样本来计算梯度并更新参数。缺点：准确率不如BGD\非全局最优化 优点：计算小批量数据的梯度更加高效、支持在线学习
```

## 8.学习率的调整策略

```
1.有序调整：等间距调整（StepLR），按需调整学习率，指数衰减调整（Exponential）和余弦退火（CosineAnnealing）
2.自适应调整：自适应调整学习率（ReduceLROnPlateau）
3.自定义调整：自定义调整学习率（LambdaLR）
```

